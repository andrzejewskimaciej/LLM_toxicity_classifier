# Wyjaśnialny klasyfikator toksyczności wypowiedzi (LLM-based)
## Cel:
Stworzyć model oparty na LLM, który klasyfikuje toksyczność krótkich wypowiedzi (np. komentarzy) i pokazuje, które fragmenty tekstu były problematyczne.

## Zakres (minimalny):

* Przygotować zbiór komentarzy z etykietami (toksyczny / nietoksyczny / graniczny).

* Zaimplementować klasyfikację LLM.

## Warstwa wyjaśnialności:

* podświetlanie w tekście słów/zwrotów odpowiedzialnych za klasyfikację,

* analiza błędnych decyzji (np. ironia, żart).

